{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08f4197c-01e7-4c2c-b1c6-cc9975913df2",
   "metadata": {},
   "source": [
    "## LOADING IMDB DATASET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "36a379bf-8120-4edc-8fb6-6588da45ccf0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files in train folder: ['labeledBow.feat', 'neg', 'pos', 'unsup', 'unsupBow.feat', 'urls_neg.txt', 'urls_pos.txt', 'urls_unsup.txt']\n",
      "Files in test folder: ['labeledBow.feat', 'neg', 'pos', 'urls_neg.txt', 'urls_pos.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_folder = \"train\"\n",
    "train_files = os.listdir(train_folder)\n",
    "print(\"Files in train folder:\", train_files)\n",
    "\n",
    "test_folder = \"test\"\n",
    "test_files = os.listdir(test_folder)\n",
    "print(\"Files in test folder:\", test_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3abe6033-4762-4baf-a692-e9c54fa1dad8",
   "metadata": {},
   "source": [
    "## DATA PRE PROCESSING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7fca0137-52fb-4f95-ade8-ce4b770da4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Displaying sample text files from train/pos:\n",
      "File: 0_9.txt\n",
      "Content:\n",
      "Bromwell High is a cartoon comedy. It ran at the same time as some other programs about school life, such as \"Teachers\". My 35 years in the teaching profession lead me to believe that Bromwell High's satire is much closer to reality than is \"Teachers\". The scramble to survive financially, the insightful students who can see right through their pathetic teachers' pomp, the pettiness of the whole situation, all remind me of the schools I knew and their students. When I saw the episode in which a student repeatedly tried to burn down the school, I immediately recalled ......... at .......... High. A classic line: INSPECTOR: I'm here to sack one of your teachers. STUDENT: Welcome to Bromwell High. I expect that many adults of my age think that Bromwell High is far fetched. What a pity that it isn't!\n",
      "\n",
      "\n",
      "Displaying sample text files from train/neg:\n",
      "File: 0_3.txt\n",
      "Content:\n",
      "Story of a man who has unnatural feelings for a pig. Starts out with a opening scene that is a terrific example of absurd comedy. A formal orchestra audience is turned into an insane, violent mob by the crazy chantings of it's singers. Unfortunately it stays absurd the WHOLE time with no general narrative eventually making it just too off putting. Even those from the era should be turned off. The cryptic dialogue would make Shakespeare seem easy to a third grader. On a technical level it's better than you might think with some good cinematography by future great Vilmos Zsigmond. Future stars Sally Kirkland and Frederic Forrest can be seen briefly.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_pos = \"train/pos\"\n",
    "train_neg = \"train/neg\"\n",
    "\n",
    "def display_sample_text(folder_path, num_samples=1):\n",
    "    print(f\"Displaying sample text files from {folder_path}:\")\n",
    "    file_names = os.listdir(folder_path)[:num_samples]\n",
    "    for file_name in file_names:\n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        with open(file_path, \"r\") as file:\n",
    "            text = file.read()\n",
    "            print(f\"File: {file_name}\")\n",
    "            print(\"Content:\")\n",
    "            print(text)\n",
    "            print(\"\\n\")\n",
    "\n",
    "display_sample_text(train_pos)\n",
    "display_sample_text(train_neg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "574dd31c-8af7-4bba-87f1-482d9ba075b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Positive Count 12500\n",
      "Negative Count 12500\n",
      "Number of training examples: 25000\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "train_folder = \"train\"\n",
    "train_data = []\n",
    "\n",
    "\n",
    "pos_folder = os.path.join(train_folder, 'pos')\n",
    "for filename in os.listdir(pos_folder):\n",
    "    with open(os.path.join(pos_folder, filename), 'r', encoding='utf-8') as file:\n",
    "        train_data.append((file.read(), 'pos'))\n",
    "\n",
    "\n",
    "neg_folder = os.path.join(train_folder, 'neg')\n",
    "for filename in os.listdir(neg_folder):\n",
    "    with open(os.path.join(neg_folder, filename), 'r', encoding='utf-8') as file:\n",
    "        train_data.append((file.read(), 'neg'))\n",
    "\n",
    "pos_folder = os.path.join(train_folder, 'pos')\n",
    "pos_files_count = len(os.listdir(pos_folder))\n",
    "print(\"Positive Count\",pos_files_count)\n",
    "neg_folder = os.path.join(train_folder, 'neg')\n",
    "neg_files_count = len(os.listdir(neg_folder))\n",
    "print(\"Negative Count\",neg_files_count)\n",
    "\n",
    "print(\"Number of training examples:\", len(train_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d78e390-64e0-4bcc-a8c7-b0ac52cda20d",
   "metadata": {},
   "source": [
    "## SPACY TOKENIZATION"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3643408d-b47c-4c3d-85bd-33baa66d9589",
   "metadata": {},
   "source": [
    "# Due to memory issue saving the tokenized training data in System Folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "070fc518-9c17-4988-bc2b-b5873fc5db56",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pos: 100%|████████████████████████████████████████████████████████████| 12500/12500 [10:50<00:00, 19.23it/s]\n",
      "Processing neg: 100%|████████████████████████████████████████████████████████████| 12500/12500 [09:38<00:00, 21.60it/s]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def tokenize_and_filter(text):\n",
    "    tokens = [token.text for token in nlp(text)]\n",
    "    token_counts = Counter(tokens)\n",
    "    filtered_tokens = [token if token_counts[token] >= 5 else \"UNK\" for token in tokens]\n",
    "    return filtered_tokens\n",
    "\n",
    "\n",
    "def process_folder(folder_path, output_folder):\n",
    "    filenames = os.listdir(folder_path)\n",
    "    for filename in tqdm(filenames, desc=f\"Processing {os.path.basename(folder_path)}\"):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            tokens = tokenize_and_filter(text)\n",
    "        \n",
    "        output_file_path = os.path.join(output_folder, filename)\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(\" \".join(tokens))\n",
    "\n",
    "\n",
    "train_folder = \"train\"\n",
    "pos_train_folder = os.path.join(train_folder, 'pos')\n",
    "neg_train_folder = os.path.join(train_folder, 'neg')\n",
    "output_train_folder = \"tokenized_train_data\"\n",
    "\n",
    "\n",
    "os.makedirs(output_train_folder, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_train_folder, 'pos'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_train_folder, 'neg'), exist_ok=True)\n",
    "\n",
    "\n",
    "process_folder(pos_train_folder, os.path.join(output_train_folder, 'pos'))\n",
    "process_folder(neg_train_folder, os.path.join(output_train_folder, 'neg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcda2d62-f3c1-479c-8de6-f445531458d4",
   "metadata": {},
   "source": [
    "## One hot encoding of training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be965716-c75a-44ad-bb9e-d8fa23a42d61",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing pos: 100%|██████████████████████████████████████████████████████████████| 6250/6250 [46:26<00:00,  2.24it/s]\n",
      "Processing neg:  33%|███████████████████▊                                        | 2059/6250 [32:53<1:09:01,  1.01it/s]"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "def load_tokenized_data(folder_path, num_files):\n",
    "    tokenized_data = []\n",
    "    filenames = os.listdir(folder_path)[:num_files]\n",
    "    for filename in filenames:\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            tokens = file.read().strip().split()\n",
    "            tokenized_data.append(tokens)\n",
    "    return tokenized_data\n",
    "\n",
    "\n",
    "def create_vocab(tokenized_data):\n",
    "    vocab = {}\n",
    "    vocab['PAD'] = 0  \n",
    "    for tokens in tokenized_data:\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "\n",
    "def one_hot_encoding(tokens, vocab, max_length):\n",
    "    one_hot_encoded = []\n",
    "    for token in tokens:\n",
    "        one_hot = [0] * len(vocab)\n",
    "        if token in vocab:\n",
    "            one_hot[vocab[token]] = 1\n",
    "        else:\n",
    "            one_hot[vocab['PAD']] = 1  \n",
    "        one_hot_encoded.append(one_hot)\n",
    "    if len(one_hot_encoded) < max_length:\n",
    "        one_hot_encoded += [[0] * len(vocab)] * (max_length - len(one_hot_encoded))\n",
    "    else:\n",
    "        one_hot_encoded = one_hot_encoded[:max_length]\n",
    "    return one_hot_encoded\n",
    "\n",
    "def process_folder_in_batches(folder_path, output_folder, vocab, max_length, batch_size):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    filenames = os.listdir(folder_path)\n",
    "    num_batches = len(filenames) // batch_size\n",
    "    for batch_idx in tqdm(range(num_batches), desc=f\"Processing {os.path.basename(folder_path)}\"):\n",
    "        batch_filenames = filenames[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
    "        batch_data = []\n",
    "        for filename in batch_filenames:\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                tokens = file.read().strip().split()\n",
    "                encoded_sentence = one_hot_encoding(tokens, vocab, max_length)\n",
    "                batch_data.append(encoded_sentence)\n",
    "        if batch_data:\n",
    "            with h5py.File(os.path.join(output_folder, f'batch_{batch_idx}.h5'), 'w') as hf:\n",
    "                hf.create_dataset('data', data=batch_data)\n",
    "        else:\n",
    "            print(f\"No data for batch {batch_idx}\")\n",
    "\n",
    "tokenized_train_folder = \"tokenized_train_data\"\n",
    "pos_tokenized_folder = os.path.join(tokenized_train_folder, 'pos')\n",
    "neg_tokenized_folder = os.path.join(tokenized_train_folder, 'neg')\n",
    "output_train_folder = \"encoded_train_data\"\n",
    "\n",
    "\n",
    "num_files = 5000  \n",
    "pos_tokenized_data = load_tokenized_data(pos_tokenized_folder, num_files)\n",
    "neg_tokenized_data = load_tokenized_data(neg_tokenized_folder, num_files)\n",
    "\n",
    "\n",
    "all_tokenized_data = pos_tokenized_data + neg_tokenized_data\n",
    "vocab = create_vocab(all_tokenized_data)\n",
    "\n",
    "\n",
    "if 'PAD' not in vocab:\n",
    "    vocab['PAD'] = len(vocab)\n",
    "\n",
    "avg_length = sum(len(tokens) for tokens in all_tokenized_data) / len(all_tokenized_data)\n",
    "max_length = int(avg_length)\n",
    "\n",
    "batch_size = 2  \n",
    "process_folder_in_batches(pos_tokenized_folder, os.path.join(output_train_folder, 'pos'), vocab, max_length, batch_size)\n",
    "process_folder_in_batches(neg_tokenized_folder, os.path.join(output_train_folder, 'neg'), vocab, max_length, batch_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23a14b0e-9d5f-43a6-8cf9-641f580f32eb",
   "metadata": {},
   "source": [
    "## Loading encoded data and converting to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1c65048-3dac-4a5b-bed9-4620585312d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def load_encoded_data_from_h5(folder_path, num_files, max_length):\n",
    "    encoded_data_list = []\n",
    "    filenames = os.listdir(folder_path)[:num_files]\n",
    "    for filename in filenames:\n",
    "        with h5py.File(os.path.join(folder_path, filename), 'r') as hf:\n",
    "            data = hf['data'][:]\n",
    "            # Ensure data is of max_length\n",
    "            if data.shape[1] < max_length:\n",
    "                padding = np.zeros((data.shape[0], max_length - data.shape[1], data.shape[2]))\n",
    "                data = np.concatenate([data, padding], axis=1)\n",
    "            encoded_data_list.append(data)\n",
    "    return np.array(encoded_data_list)\n",
    "\n",
    "encoded_train_folder_pos = \"encoded_train_data/pos\"\n",
    "encoded_train_folder_neg = \"encoded_train_data/neg\"\n",
    "\n",
    "\n",
    "max_length = 10\n",
    "num_files_to_load = 100\n",
    "for folder_path in [encoded_train_folder_pos, encoded_train_folder_neg]:\n",
    "    filenames = os.listdir(folder_path)[:num_files_to_load]\n",
    "    for filename in filenames:\n",
    "        with h5py.File(os.path.join(folder_path, filename), 'r') as hf:\n",
    "            data = hf['data'][:]\n",
    "            max_length = max(max_length, data.shape[1])\n",
    "\n",
    "print(\"Maximum length:\", max_length)\n",
    "\n",
    "\n",
    "batch_size = 2  \n",
    "num_batches = num_files_to_load // batch_size\n",
    "\n",
    "encoded_data_pos_list = []\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, num_files_to_load)\n",
    "    encoded_data_pos_batch = load_encoded_data_from_h5(encoded_train_folder_pos, end_idx - start_idx, max_length)\n",
    "    encoded_data_pos_list.append(encoded_data_pos_batch)\n",
    "\n",
    "encoded_data_neg_list = []\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, num_files_to_load)\n",
    "    encoded_data_neg_batch = load_encoded_data_from_h5(encoded_train_folder_neg, end_idx - start_idx, max_length)\n",
    "    encoded_data_neg_list.append(encoded_data_neg_batch)\n",
    "\n",
    "encoded_data_pos = np.concatenate(encoded_data_pos_list, axis=0)\n",
    "encoded_data_neg = np.concatenate(encoded_data_neg_list, axis=0)\n",
    "\n",
    "\n",
    "print(\"Shape of encoded data (positive):\", encoded_data_pos.shape)\n",
    "print(\"Shape of encoded data (negative):\", encoded_data_neg.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19822594-5dc5-4567-a565-dcdbb0f40c20",
   "metadata": {},
   "source": [
    "## DEMO NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6566ca4c-52af-4bc8-89b9-96c348dc5d35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TwoHiddenLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(TwoHiddenLayerNN, self).__init__()\n",
    "        self.hidden_layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.hidden_layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.output_layer = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.hidden_layer1(x))\n",
    "        x = F.relu(self.hidden_layer2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "input_size = 2283\n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 128\n",
    "output_size = 2  \n",
    "\n",
    "model = TwoHiddenLayerNN(input_size, hidden_size1, hidden_size2, output_size)\n",
    "\n",
    "\n",
    "inputs = torch.randn(100, input_size).float()  \n",
    "\n",
    "outputs = model(inputs)\n",
    "\n",
    "print(\"Shape of output tensor:\", outputs.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f25821b-9ef9-494e-90d7-40ca14765f92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "000a8e61-3921-4b95-9a0a-e84467c35468",
   "metadata": {},
   "source": [
    "## Feed Forward NN with two hidden layer of size 256 and 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a46dca3-f1c2-43ac-94a0-1f5081a438fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "class TwoHiddenLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(TwoHiddenLayerNN, self).__init__()\n",
    "        self.hidden_layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.hidden_layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.output_layer = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden_layer1(x))\n",
    "        x = torch.relu(self.hidden_layer2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "\n",
    "input_size = 667389  \n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 128\n",
    "output_size = 2 \n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = TwoHiddenLayerNN(input_size, hidden_size1, hidden_size2, output_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def load_encoded_data_from_h5(folder_path, num_files, max_length):\n",
    "    encoded_data_list = []\n",
    "    filenames = os.listdir(folder_path)[:num_files]\n",
    "    for filename in filenames:\n",
    "        with h5py.File(os.path.join(folder_path, filename), 'r') as hf:\n",
    "            data = hf['data'][:]\n",
    "            if data.shape[1] < max_length:\n",
    "                padding = np.zeros((data.shape[0], max_length - data.shape[1], data.shape[2]))\n",
    "                data = np.concatenate([data, padding], axis=1)\n",
    "            encoded_data_list.append(data)\n",
    "    return np.array(encoded_data_list)\n",
    "\n",
    "encoded_train_folder_pos = \"encoded_train_data/pos\"\n",
    "encoded_train_folder_neg = \"encoded_train_data/neg\"\n",
    "num_files_to_load = 50 \n",
    "max_length = 269  \n",
    "\n",
    "encoded_data_pos = load_encoded_data_from_h5(encoded_train_folder_pos, num_files_to_load, max_length)\n",
    "encoded_data_neg = load_encoded_data_from_h5(encoded_train_folder_neg, num_files_to_load, max_length)\n",
    "\n",
    "assert encoded_data_pos.shape[0] == num_files_to_load\n",
    "assert encoded_data_neg.shape[0] == num_files_to_load\n",
    "\n",
    "inputs_pos = encoded_data_pos.reshape(-1, 269 * 2481)\n",
    "inputs_neg = encoded_data_neg.reshape(-1, 269 * 2481)\n",
    "\n",
    "labels_pos = np.ones((inputs_pos.shape[0],), dtype=np.int64)\n",
    "labels_neg = np.zeros((inputs_neg.shape[0],), dtype=np.int64)\n",
    "\n",
    "inputs = np.concatenate((inputs_pos, inputs_neg), axis=0)\n",
    "labels = np.concatenate((labels_pos, labels_neg), axis=0)\n",
    "\n",
    "inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(inputs, labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06514b55-0e1d-4c5a-a3ab-90c3b1ac0085",
   "metadata": {},
   "source": [
    "##  Validation on 10% of training data¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5085914e-e3c0-40ee-a038-66a4c1b40365",
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_ratio = 0.1\n",
    "num_validation = int(len(train_dataset) * validation_ratio)\n",
    "num_training = len(train_dataset) - num_validation\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [num_training, num_validation])\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval()  \n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {(100 * correct / total):.2f}%\")\n",
    "\n",
    "    model.train() \n",
    "\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da84cdf-1a6d-4896-9eb5-6d32e0012bb9",
   "metadata": {},
   "source": [
    "## Evaluation metrices of both the labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7a8801c-f5d3-42a9-8d27-4a6c6ee84889",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "validation_ratio = 0.1\n",
    "num_validation = int(len(train_dataset) * validation_ratio)\n",
    "num_training = len(train_dataset) - num_validation\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [num_training, num_validation])\n",
    "\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "accuracies = []\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1_scores = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    model.eval() \n",
    "    val_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_predicted_labels = []\n",
    "    all_true_labels = []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in val_loader:\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            all_predicted_labels.extend(predicted.cpu().numpy())\n",
    "            all_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    accuracy = (correct / total) * 100\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(all_true_labels, all_predicted_labels, average=None)\n",
    "\n",
    "    train_losses.append(running_loss/len(train_loader))\n",
    "    val_losses.append(val_loss/len(val_loader))\n",
    "    accuracies.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1_scores.append(f1)\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {running_loss/len(train_loader):.4f}, Validation Loss: {val_loss/len(val_loader):.4f}, Validation Accuracy: {accuracy:.2f}%\")\n",
    "    for label in range(output_size):\n",
    "        print(f\"Label {label}: Precision={precision[label]:.4f}, Recall={recall[label]:.4f}, F1-Score={f1[label]:.4f}\")\n",
    "    model.train() \n",
    "\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a14edd16-e3d5-499d-8444-36814fb25d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label='Training Loss')\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Losses')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560c5ccc-e4c3-4692-b45b-16895d5c9dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), accuracies, label='Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.title('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53a55aad-19de-41c5-88c9-30d8a1c6ceb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "labels = [f'Label {i}' for i in range(output_size)]\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(labels, precisions[-1], label='Precision')\n",
    "plt.plot(labels, recalls[-1], label='Recall')\n",
    "plt.plot(labels, f1_scores[-1], label='F1-Score')\n",
    "plt.xlabel('Labels')\n",
    "plt.ylabel('Metrics')\n",
    "plt.title('Precision, Recall, and F1-Score for the Last Epoch')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deca5141-f1f8-4b1c-b68f-29fdb20a9a2c",
   "metadata": {},
   "source": [
    "## SPACY TOKENIZATION OF TEST DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92c3ebe1-8b45-4987-94d9-6221b0fc4959",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_and_filter(text):\n",
    "    tokens = [token.text for token in nlp(text)]\n",
    "    token_counts = Counter(tokens)\n",
    "    filtered_tokens = [token if token_counts[token] >= 5 else \"UNK\" for token in tokens]\n",
    "    return filtered_tokens\n",
    "\n",
    "def process_folder(folder_path, output_folder, file_limit=10):\n",
    "    filenames = os.listdir(folder_path)[:file_limit]\n",
    "    for filename in tqdm(filenames, desc=f\"Processing {os.path.basename(folder_path)}\"):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            tokens = tokenize_and_filter(text)\n",
    "        \n",
    "        output_file_path = os.path.join(output_folder, filename)\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(\" \".join(tokens))\n",
    "\n",
    "train_folder = \"C:/Users/Deepjyoti Bodo/Downloads/Datasets/aclImdb/test\"\n",
    "pos_train_folder = os.path.join(train_folder, 'pos')\n",
    "neg_train_folder = os.path.join(train_folder, 'neg')\n",
    "output_train_folder = \"tokenized_test_data\"\n",
    "\n",
    "os.makedirs(output_train_folder, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_train_folder, 'pos'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_train_folder, 'neg'), exist_ok=True)\n",
    "\n",
    "file_limit = 10\n",
    "process_folder(pos_train_folder, os.path.join(output_train_folder, 'pos'), file_limit)\n",
    "process_folder(neg_train_folder, os.path.join(output_train_folder, 'neg'), file_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf9fb5b-5cf5-4066-a798-e0c2f26d2def",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "def load_tokenized_data(folder_path, num_files):\n",
    "    tokenized_data = []\n",
    "    filenames = os.listdir(folder_path)[:num_files]\n",
    "    for filename in filenames:\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            tokens = file.read().strip().split()\n",
    "            tokenized_data.append(tokens)\n",
    "    return tokenized_data\n",
    "\n",
    "def create_vocab(tokenized_data):\n",
    "    vocab = {}\n",
    "    vocab['PAD'] = 0  \n",
    "    for tokens in tokenized_data:\n",
    "        for token in tokens:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)\n",
    "    return vocab\n",
    "\n",
    "def one_hot_encoding(tokens, vocab, max_length):\n",
    "    one_hot_encoded = []\n",
    "    for token in tokens:\n",
    "        one_hot = [0] * len(vocab)\n",
    "        if token in vocab:\n",
    "            one_hot[vocab[token]] = 1\n",
    "        else:\n",
    "            one_hot[vocab['PAD']] = 1  \n",
    "        one_hot_encoded.append(one_hot)\n",
    "    if len(one_hot_encoded) < max_length:\n",
    "        one_hot_encoded += [[0] * len(vocab)] * (max_length - len(one_hot_encoded))\n",
    "    else:\n",
    "        one_hot_encoded = one_hot_encoded[:max_length]\n",
    "    return one_hot_encoded\n",
    "\n",
    "def process_folder_in_batches(folder_path, output_folder, vocab, max_length, batch_size):\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "    filenames = os.listdir(folder_path)\n",
    "    num_batches = len(filenames) // batch_size\n",
    "    for batch_idx in tqdm(range(num_batches), desc=f\"Processing {os.path.basename(folder_path)}\"):\n",
    "        batch_filenames = filenames[batch_idx * batch_size: (batch_idx + 1) * batch_size]\n",
    "        batch_data = []\n",
    "        for filename in batch_filenames:\n",
    "            with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "                tokens = file.read().strip().split()\n",
    "                encoded_sentence = one_hot_encoding(tokens, vocab, max_length)\n",
    "                batch_data.append(encoded_sentence)\n",
    "        if batch_data:\n",
    "            with h5py.File(os.path.join(output_folder, f'batch_{batch_idx}.h5'), 'w') as hf:\n",
    "                hf.create_dataset('data', data=batch_data)\n",
    "        else:\n",
    "            print(f\"No data for batch {batch_idx}\")\n",
    "\n",
    "tokenized_test_folder = \"tokenized_test_data\"\n",
    "pos_tokenized_test_folder = os.path.join(tokenized_test_folder, 'pos')\n",
    "neg_tokenized_test_folder = os.path.join(tokenized_test_folder, 'neg')\n",
    "output_test_folder = \"encoded_test_data\"\n",
    "\n",
    "num_files_test = 20  \n",
    "pos_tokenized_test_data = load_tokenized_data(pos_tokenized_test_folder, num_files_test)\n",
    "neg_tokenized_test_data = load_tokenized_data(neg_tokenized_test_folder, num_files_test)\n",
    "\n",
    "all_tokenized_test_data = pos_tokenized_test_data + neg_tokenized_test_data\n",
    "vocab = create_vocab(all_tokenized_test_data)\n",
    "\n",
    "if 'PAD' not in vocab:\n",
    "    vocab['PAD'] = len(vocab)\n",
    "\n",
    "avg_length_test = sum(len(tokens) for tokens in all_tokenized_test_data) / len(all_tokenized_test_data)\n",
    "max_length_test = int(avg_length_test)\n",
    "\n",
    "batch_size_test = 1  \n",
    "process_folder_in_batches(pos_tokenized_test_folder, os.path.join(output_test_folder, 'pos'), vocab, max_length_test, batch_size_test)\n",
    "process_folder_in_batches(neg_tokenized_test_folder, os.path.join(output_test_folder, 'neg'), vocab, max_length_test, batch_size_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4887bff0-6854-4db3-af47-5951f96632ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def tokenize_and_filter(text):\n",
    "    tokens = [token.text for token in nlp(text)]\n",
    "    token_counts = Counter(tokens)\n",
    "    filtered_tokens = [token if token_counts[token] >= 5 else \"UNK\" for token in tokens]\n",
    "    return filtered_tokens\n",
    "\n",
    "def process_folder(folder_path, output_folder):\n",
    "    filenames = os.listdir(folder_path)\n",
    "    for filename in tqdm(filenames, desc=f\"Processing {os.path.basename(folder_path)}\"):\n",
    "        with open(os.path.join(folder_path, filename), 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "            tokens = tokenize_and_filter(text)\n",
    "        output_file_path = os.path.join(output_folder, filename)\n",
    "        with open(output_file_path, 'w', encoding='utf-8') as output_file:\n",
    "            output_file.write(\" \".join(tokens))\n",
    "\n",
    "train_folder = \"test\"\n",
    "pos_train_folder = os.path.join(train_folder, 'pos')\n",
    "neg_train_folder = os.path.join(train_folder, 'neg')\n",
    "output_train_folder = \"tokenized_test_data\"\n",
    "\n",
    "os.makedirs(output_train_folder, exist_ok=True)\n",
    "os.makedirs(os.path.join(output_train_folder, 'pos'), exist_ok=True)\n",
    "os.makedirs(os.path.join(output_train_folder, 'neg'), exist_ok=True)\n",
    "\n",
    "process_folder(pos_train_folder, os.path.join(output_train_folder, 'pos'))\n",
    "process_folder(neg_train_folder, os.path.join(output_train_folder, 'neg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1678dc5d-bd02-43ba-beb8-d42b52aa8348",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "\n",
    "def load_encoded_data_from_h5(folder_path, num_files, max_length):\n",
    "    encoded_data_list = []\n",
    "    filenames = os.listdir(folder_path)[:num_files]\n",
    "    for filename in filenames:\n",
    "        with h5py.File(os.path.join(folder_path, filename), 'r') as hf:\n",
    "            data = hf['data'][:]\n",
    "            if data.shape[1] < max_length:\n",
    "                padding = np.zeros((data.shape[0], max_length - data.shape[1], data.shape[2]))\n",
    "                data = np.concatenate([data, padding], axis=1)\n",
    "            encoded_data_list.append(data)\n",
    "    return np.array(encoded_data_list)\n",
    "\n",
    "\n",
    "encoded_train_folder_pos = \"encoded_test_data/pos\"\n",
    "encoded_train_folder_neg = \"encoded_test_data/neg\"\n",
    "\n",
    "max_length = 0\n",
    "num_files_to_load = 100\n",
    "for folder_path in [encoded_train_folder_pos, encoded_train_folder_neg]:\n",
    "    filenames = os.listdir(folder_path)[:num_files_to_load]\n",
    "    for filename in filenames:\n",
    "        with h5py.File(os.path.join(folder_path, filename), 'r') as hf:\n",
    "            data = hf['data'][:]\n",
    "            max_length = max(max_length, data.shape[1])\n",
    "\n",
    "print(\"Maximum length:\", max_length)\n",
    "\n",
    "batch_size = 2  \n",
    "num_batches = num_files_to_load // batch_size\n",
    "\n",
    "encoded_data_pos_list = []\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, num_files_to_load)\n",
    "    encoded_data_pos_batch = load_encoded_data_from_h5(encoded_train_folder_pos, end_idx - start_idx, max_length)\n",
    "    encoded_data_pos_list.append(encoded_data_pos_batch)\n",
    "\n",
    "encoded_data_neg_list = []\n",
    "for batch_idx in range(num_batches):\n",
    "    start_idx = batch_idx * batch_size\n",
    "    end_idx = min((batch_idx + 1) * batch_size, num_files_to_load)\n",
    "    encoded_data_neg_batch = load_encoded_data_from_h5(encoded_train_folder_neg, end_idx - start_idx, max_length)\n",
    "    encoded_data_neg_list.append(encoded_data_neg_batch)\n",
    "\n",
    "encoded_data_pos = np.concatenate(encoded_data_pos_list, axis=0)\n",
    "encoded_data_neg = np.concatenate(encoded_data_neg_list, axis=0)\n",
    "\n",
    "print(\"Shape of encoded data (positive):\", encoded_data_pos.shape)\n",
    "print(\"Shape of encoded data (negative):\", encoded_data_neg.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1fd8e-ba26-4859-add9-977679619117",
   "metadata": {},
   "source": [
    "## Loading test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e727313-e35e-4ed5-b86b-53de5ef0f266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "def load_encoded_test_data_from_h5(folder_path):\n",
    "    encoded_data_list = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        with h5py.File(os.path.join(folder_path, filename), 'r') as hf:\n",
    "            data = hf['data'][:]\n",
    "            encoded_data_list.append(data)\n",
    "    return encoded_data_list\n",
    "\n",
    "encoded_test_folder_pos = \"encoded_test_data/pos\"\n",
    "encoded_test_folder_neg = \"encoded_test_data/neg\"\n",
    "\n",
    "encoded_test_data_pos = load_encoded_test_data_from_h5(encoded_test_folder_pos)\n",
    "encoded_test_data_neg = load_encoded_test_data_from_h5(encoded_test_folder_neg)\n",
    "\n",
    "inputs_test_pos = torch.tensor(encoded_test_data_pos, dtype=torch.float32)\n",
    "inputs_test_neg = torch.tensor(encoded_test_data_neg, dtype=torch.float32)\n",
    "\n",
    "total_elements_pos = inputs_test_pos.numel()\n",
    "total_elements_neg = inputs_test_neg.numel()\n",
    "\n",
    "print(\"Total elements in inputs_test_pos:\", total_elements_pos)\n",
    "print(\"Total elements in inputs_test_neg:\", total_elements_neg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc5174fa-6edf-45f5-b650-a805d0575e07",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_train_folder_pos = \"encoded_test_data/pos\"\n",
    "encoded_train_folder_neg = \"encoded_test_data/neg\"\n",
    "num_files_to_load = 50 \n",
    "max_length = 263  \n",
    "\n",
    "encoded_data_pos = load_encoded_data_from_h5(encoded_train_folder_pos, num_files_to_load, max_length)\n",
    "encoded_data_neg = load_encoded_data_from_h5(encoded_train_folder_neg, num_files_to_load, max_length)\n",
    "\n",
    "print(\"Shape of encoded_data_pos before reshaping:\", encoded_data_pos.shape)\n",
    "print(\"Shape of encoded_data_neg before reshaping:\", encoded_data_neg.shape)\n",
    "\n",
    "\n",
    "expected_size = num_files_to_load * 2 * max_length * input_size\n",
    "print(\"Expected size after reshaping:\", expected_size)\n",
    "\n",
    "inputs_pos = encoded_data_pos.reshape(-1, 2 * 263 * 2283)\n",
    "inputs_neg = encoded_data_neg.reshape(-1, 2 * 263 * 2283)\n",
    "\n",
    "print(\"Shape of inputs_pos after reshaping:\", inputs_pos.shape)\n",
    "print(\"Shape of inputs_neg after reshaping:\", inputs_neg.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac71500-a72c-4429-b40f-654964cc7528",
   "metadata": {},
   "source": [
    "## Testing the model on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aebb067-df0d-4948-88d7-8488dfba9e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "\n",
    "class TwoHiddenLayerNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size1, hidden_size2, output_size):\n",
    "        super(TwoHiddenLayerNN, self).__init__()\n",
    "        self.hidden_layer1 = nn.Linear(input_size, hidden_size1)\n",
    "        self.hidden_layer2 = nn.Linear(hidden_size1, hidden_size2)\n",
    "        self.output_layer = nn.Linear(hidden_size2, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.hidden_layer1(x))\n",
    "        x = torch.relu(self.hidden_layer2(x))\n",
    "        x = self.output_layer(x)\n",
    "        return x\n",
    "input_size = 600429 \n",
    "hidden_size1 = 256\n",
    "hidden_size2 = 128\n",
    "output_size = 2 \n",
    "learning_rate = 0.001\n",
    "batch_size = 32\n",
    "num_epochs = 10\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = TwoHiddenLayerNN(input_size, hidden_size1, hidden_size2, output_size).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "def load_encoded_data_from_h5(folder_path, num_files, max_length):\n",
    "    encoded_data_list = []\n",
    "    filenames = os.listdir(folder_path)[:num_files]\n",
    "    for filename in filenames:\n",
    "        with h5py.File(os.path.join(folder_path, filename), 'r') as hf:\n",
    "            data = hf['data'][:]\n",
    "            if data.shape[1] < max_length:\n",
    "                padding = np.zeros((data.shape[0], max_length - data.shape[1], data.shape[2]))\n",
    "                data = np.concatenate([data, padding], axis=1)\n",
    "            encoded_data_list.append(data)\n",
    "    return np.array(encoded_data_list)\n",
    "\n",
    "encoded_train_folder_pos = \"encoded_test_data/pos\"\n",
    "encoded_train_folder_neg = \"encoded_test_data/neg\"\n",
    "num_files_to_load = 50 \n",
    "max_length = 263 \n",
    "\n",
    "encoded_data_pos = load_encoded_data_from_h5(encoded_train_folder_pos, num_files_to_load, max_length)\n",
    "encoded_data_neg = load_encoded_data_from_h5(encoded_train_folder_neg, num_files_to_load, max_length)\n",
    "\n",
    "assert encoded_data_pos.shape[0] == num_files_to_load\n",
    "assert encoded_data_neg.shape[0] == num_files_to_load\n",
    "\n",
    "inputs_pos = encoded_data_pos.reshape(-1, 263 * 2283)\n",
    "inputs_neg = encoded_data_neg.reshape(-1, 263 * 2283)  # Corrected reshaping\n",
    "\n",
    "labels_pos = np.ones((inputs_pos.shape[0],), dtype=np.int64)\n",
    "labels_neg = np.zeros((inputs_neg.shape[0],), dtype=np.int64)\n",
    "\n",
    "inputs = np.concatenate((inputs_pos, inputs_neg), axis=0)\n",
    "labels = np.concatenate((labels_pos, labels_neg), axis=0)\n",
    "\n",
    "inputs = torch.tensor(inputs, dtype=torch.float32).to(device)\n",
    "labels = torch.tensor(labels, dtype=torch.long).to(device)\n",
    "\n",
    "train_dataset = TensorDataset(inputs, labels)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for inputs, labels in train_loader:\n",
    "        optimizer.zero_grad() \n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")\n",
    "\n",
    "print(\"Training finished!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebbe0cfb-1d78-4d6a-8ce4-c3278df025da",
   "metadata": {},
   "source": [
    "## EXtra\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbbac760-6e9a-4d34-854a-aff34c47205f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.bilstm1 = nn.LSTM(embed_size, hidden_size, batch_first=True, bidirectional=True)\n",
    "        self.bilstm2 = nn.LSTM(hidden_size * 2, hidden_size // 2, batch_first=True, bidirectional=True)\n",
    "        self.fc1 = nn.Linear(hidden_size * 2, 1024)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.fc2 = nn.Linear(1024, 512)\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.fc3 = nn.Linear(512, 256)\n",
    "        self.dropout3 = nn.Dropout(0.25)\n",
    "        self.fc4 = nn.Linear(256, 128)\n",
    "        self.dropout4 = nn.Dropout(0.25)\n",
    "        self.fc5 = nn.Linear(128, 64)\n",
    "        self.dropout5 = nn.Dropout(0.25)\n",
    "        self.fc6 = nn.Linear(64, 4)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm1_out, _ = self.bilstm1(embedded)\n",
    "        lstm2_out, _ = self.bilstm2(lstm1_out)\n",
    "        avg_pooled = torch.mean(lstm2_out, dim=1)\n",
    "        fc1_out = self.dropout1(torch.relu(self.fc1(avg_pooled)))\n",
    "        fc2_out = self.dropout2(torch.relu(self.fc2(fc1_out)))\n",
    "        fc3_out = self.dropout3(torch.relu(self.fc3(fc2_out)))\n",
    "        fc4_out = self.dropout4(torch.relu(self.fc4(fc3_out)))\n",
    "        fc5_out = self.dropout5(torch.relu(self.fc5(fc4_out)))\n",
    "        output = self.fc6(fc5_out)\n",
    "        return self.softmax(output)\n",
    "\n",
    "\n",
    "vocab_size =12000\n",
    "embed_size = 100\n",
    "hidden_size = 256 \n",
    "model = BiLSTMModel(vocab_size, embed_size, hidden_size)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6a8e60-0c96-4326-b287-9ad0efcec99a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "user_info": {
   "email": "deepbodo5@gmail.com",
   "name": "Deepjyoti Bodo"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
